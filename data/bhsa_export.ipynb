{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Export\n",
    "\n",
    "## \\*Updates\\*\n",
    "\n",
    "### 10 October\n",
    "\n",
    "After looking at initial data, we decided to refine our dataset to exclude clauses with certain phrase functions based on the counts below:\n",
    "\n",
    "```\n",
    "EBH [('IntS', 1.0),\n",
    " ('Ques', 2.0),\n",
    " ('Exst', 4.0),\n",
    " ('ModS', 4.0),\n",
    " ('NCoS', 4.0),\n",
    " ('NCop', 13.0),\n",
    " ('Supp', 13.0),\n",
    " ('PrAd', 16.0),\n",
    " ('Frnt', 26.0),\n",
    " ('Intj', 55.0),\n",
    " ('PreS', 74.0),\n",
    " ('Nega', 105.0),\n",
    " ('Modi', 146.0),\n",
    " ('PreO', 179.0),\n",
    " ('Time', 236.0),\n",
    " ('Loca', 250.0),\n",
    " ('Rela', 295.0),\n",
    " ('Adju', 355.0),\n",
    " ('PreC', 788.0),\n",
    " ('Objc', 1601.0),\n",
    " ('Cmpl', 1977.0),\n",
    " ('Subj', 2243.0),\n",
    " ('Conj', 4052.0),\n",
    " ('Pred', 4147.0)]\n",
    "\n",
    "LBH [('Frnt', 1.0),\n",
    " ('ModS', 1.0),\n",
    " ('Supp', 1.0),\n",
    " ('PrAd', 3.0),\n",
    " ('Ques', 3.0),\n",
    " ('PreS', 4.0),\n",
    " ('NCop', 5.0),\n",
    " ('Modi', 9.0),\n",
    " ('Loca', 11.0),\n",
    " ('Nega', 11.0),\n",
    " ('PreO', 19.0),\n",
    " ('Time', 35.0),\n",
    " ('Rela', 53.0),\n",
    " ('Adju', 61.0),\n",
    " ('PreC', 89.0),\n",
    " ('Objc', 112.0),\n",
    " ('Subj', 192.0),\n",
    " ('Cmpl', 209.0),\n",
    " ('Conj', 288.0),\n",
    " ('Pred', 315.0)]\n",
    "\n",
    "```\n",
    "\n",
    "As can be seen, certain functions are quite rare in the dataset. It is also not entirely clear why some functions like `Ques` are included in narratival clauses. To enable our predictions to be more focused, we now only export clauses with the following functions:\n",
    "\n",
    "```\n",
    "('IntS', 1.0),\n",
    " ('Ques', 2.0),\n",
    " ('Exst', 4.0),\n",
    " ('ModS', 4.0),\n",
    " ('NCoS', 4.0),\n",
    " ('NCop', 13.0),\n",
    " ('Supp', 13.0),\n",
    " ('PrAd', 16.0),\n",
    " ('Frnt', 26.0),\n",
    " ('Intj', 55.0),\n",
    "```\n",
    "\n",
    "Finally, we also export clauses based on individual books instead of their supposed dating.\n",
    "\n",
    "\n",
    "### 3 October\n",
    "In this notebook we export a series of .txt files containing a variety of different data sets. For our purposes here, we export narrative texts only from two general classes of texts: texts traditionally labeled as \"Early Biblical Hebrew\" and texts considered \"Late Biblical Hebrew\".\n",
    "\n",
    "We will export here the following type(s) of data:\n",
    "1. [phrase constituent](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/pdp) functions (words, also known as \"part of speech\") per clause in late/early Biblical Hebrew sources.\n",
    "\n",
    "\n",
    "The data is accessed using [Text-Fabric](https://github.com/ETCBC/text-fabric), a python package made specially for accessing copora like the ETCBC Hebrew database. \n",
    "\n",
    "## Load Text-Fabric and ETCBC Syntactic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf.fabric import Fabric # for Text-Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 3.0.9\n",
      "Api reference : https://github.com/Dans-labs/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/Dans-labs/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Example data  : https://github.com/Dans-labs/text-fabric-data\n",
      "\n",
      "113 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "# instantiate Text-Fabric (TF) objects\n",
    "\n",
    "TF = Fabric(locations='~/github/etcbc/bhsa/tf', modules='c') # load ETCBC Hebrew database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.04s B book                 from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B chapter              from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.01s B verse                from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.25s B typ                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.15s B pdp                  from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.09s B function             from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.02s B domain               from /Users/Cody/github/etcbc/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 4 for edges; 1 configs; 7 computed\n",
      "  6.40s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# load features for linguistic objects (i.e. clauses, phrases, words) from the database\n",
    "\n",
    "# features loaded in a string, space separated\n",
    "api = TF.load('''\n",
    "              book chapter verse\n",
    "              typ pdp function\n",
    "              domain\n",
    "              ''')\n",
    "\n",
    "# TF classes are globalized for easier use\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather, Arrange, and Export Data\n",
    "\n",
    "ETCBC data is stored in graph structure with linguistic objects existing as nodes that have corresponding features. TF uses a node integer to access a dictionary and pull the requested feature with a function: `F.feature.v(node_number)`. There are various other functions used to iterate through the nodes which you can explore more thoroughly in the tutorial [here](https://github.com/codykingham/tfNotebooks/blob/master/timeSpans/Text_Fabric_Tutorial.ipynb) or [here](https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb). There are also edge relationships between some nodes (such as clause relations which represent the discourse structure of the text).\n",
    "\n",
    "\n",
    "### Functions for Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_hebrew = {'Genesis', 'Exodus', 'Leviticus', \n",
    "                'Deuteronomy', 'Joshua', 'Judges',\n",
    "                '1_Samuel', '2_Samuel', '1_Kings',\n",
    "                '2_Kings'}\n",
    "\n",
    "late_hebrew = {'Esther', 'Ezra', 'Nehemiah',\n",
    "               '1_Chronicles', '2_Chronicles'}\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    '''\n",
    "    Returns dictionary with section (book, dating) as key and list as value.\n",
    "    List contains space-separated strings of word/phrase level functions.\n",
    "    Requires the feature and ETCBC object type.\n",
    "    '''\n",
    "    \n",
    "    function_data = collections.defaultdict(list)\n",
    "\n",
    "    # exclude these functions from the export (as of 10.10)\n",
    "    exclude = {'IntS', 'Ques', 'Exst', 'ModS',\n",
    "                 'NCoS', 'NCop', 'Supp', 'PrAd',\n",
    "                 'Frnt', 'Intj'}\n",
    "    \n",
    "    \n",
    "    for book in F.otype.s('book'):\n",
    "\n",
    "        # skip extraneous books\n",
    "        if T.sectionFromNode(book)[0] not in early_hebrew | late_hebrew:            \n",
    "            continue\n",
    "        \n",
    "        # set the tag under which individual files are exported\n",
    "        # i.e. this will determine the name of the files\n",
    "        book_tag = F.book.v(book)\n",
    "\n",
    "        book_clauses = [clause for clause in L.d(book, otype='clause')]\n",
    "\n",
    "        \n",
    "        # Restrictions on clauses here:\n",
    "        # get all clauses in the book. The Clauses must domain of NARRATIVE\n",
    "        # exclude clauses with certain functions\n",
    "        export_clauses = [cl for cl in book_clauses\n",
    "                             if F.domain.v(cl) == 'N'\n",
    "                             and not set(F.function.v(p) for p in L.d(cl, otype='phrase')) & exclude\n",
    "                         ]\n",
    "                \n",
    "            \n",
    "        # add phrase data per clause\n",
    "        for clause in export_clauses:\n",
    "\n",
    "            # format data for all phrases in the clause\n",
    "            phrase_functions = [F.function.v(phrase) for phrase in L.d(clause, otype='phrase')]\n",
    "            phrase_funct_str = ' '.join(phrase_functions)\n",
    "\n",
    "            function_data[book_tag].append(phrase_funct_str) # save data\n",
    "            \n",
    "    return(function_data)\n",
    "     \n",
    "    \n",
    "def export_dated_files(data_dict, file_name):\n",
    "    \n",
    "    '''\n",
    "    Exports simple data txt files per dated text.\n",
    "    '''\n",
    "    \n",
    "    for section, linguistic_data in data_dict.items():\n",
    "\n",
    "        filename = file_name.format(section)\n",
    "\n",
    "        with open(filename, 'w') as outfile:\n",
    "\n",
    "            for phrase in linguistic_data:\n",
    "                outfile.write(phrase+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clause Constituents (phrases and their functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conj Pred Subj'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply function\n",
    "phrase_function_data = get_data()\n",
    "phrase_function_data['Genesis'][0] # sample of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test before exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adju',\n",
       " 'Cmpl',\n",
       " 'Conj',\n",
       " 'Loca',\n",
       " 'Modi',\n",
       " 'Nega',\n",
       " 'Objc',\n",
       " 'PreC',\n",
       " 'PreO',\n",
       " 'PreS',\n",
       " 'Pred',\n",
       " 'PtcO',\n",
       " 'Rela',\n",
       " 'Subj',\n",
       " 'Time'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "represented_functions = set()\n",
    "\n",
    "for book, clause_dat in phrase_function_data.items():\n",
    "    \n",
    "    for clause in clause_dat:\n",
    "        \n",
    "        represented_functions |= set(clause.split())\n",
    "    \n",
    "represented_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Genesis', 'Exodus', 'Leviticus', 'Deuteronomium', 'Josua', 'Judices', 'Samuel_I', 'Samuel_II', 'Reges_I', 'Reges_II', 'Esther', 'Esra', 'Nehemia', 'Chronica_I', 'Chronica_II'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_function_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export file\n",
    "export_dated_files(phrase_function_data, 'phrase_functions/phrase_functions_{}.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
