{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, re, collections, operator, scipy.stats, sklearn.metrics\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from discreteMarkovChain import markovChain\n",
    "from tf.fabric import Fabric\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.06s B otype                from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.01s B book                 from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.00s B chapter              from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.01s B verse                from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.06s B function             from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.02s B domain               from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.20s B typ                  from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.14s B pdp                  from C:/Users/etien/Documents/github/bhsa/tf/c\n",
      "   |     0.00s Feature overview: 108 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  5.08s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='C:/Users/etien/Documents/github/bhsa/tf', modules='c', silent=True)\n",
    "\n",
    "api = TF.load('''\n",
    "              otype\n",
    "              book chapter verse\n",
    "              function domain\n",
    "              typ pdp\n",
    "              ''')\n",
    "\n",
    "api.makeAvailableIn(globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 88100 clauses loaded in data...\n"
     ]
    }
   ],
   "source": [
    "lbh_books = {'1_Chronicles', '2_Chronicles', \n",
    "             'Ezra', 'Esther', 'Nehemiah'}\n",
    "\n",
    "sbh_books = {'Genesis', 'Exodus','Leviticus', \n",
    "             'Deuteronomy','Joshua', 'Judges', \n",
    "             '1_Kings', '2_Kings', '1_Samuel',\n",
    "             '2_Samuel'}\n",
    "\n",
    "# don't load clauses with these functions\n",
    "exclude_functions = {'IntS', 'Ques', 'Exst', 'ModS',\n",
    "                     'NCoS', 'NCop', 'Supp', 'PrAd',\n",
    "                     'Frnt', 'Intj', 'EPPr', \"ExsS\", \n",
    "                     \"PrcS\", \"Voct\"}\n",
    "#exclude_functions = set()\n",
    "data = collections.defaultdict(list) # lbh, sbh, and individual books go in here\n",
    "       \n",
    "for i, clause in enumerate(F.otype.s('clause')):\n",
    "    \n",
    "    book, chapter, verse = T.sectionFromNode(clause)\n",
    "    \n",
    "    # skip clauses in our corpora\n",
    "    if book not in (lbh_books | sbh_books):\n",
    "        continue\n",
    "    # skip non-narrative clauses\n",
    "    if F.domain.v(clause) != 'N':\n",
    "        continue\n",
    "        \n",
    "    clause_phrases = L.d(clause, otype='phrase')\n",
    "    phrase_functions = [F.function.v(phrase) for phrase in clause_phrases]\n",
    "    \n",
    "    # skip if clause has excluded function\n",
    "    if set(phrase_functions) & exclude_functions:\n",
    "        continue\n",
    "        \n",
    "    if book in lbh_books:\n",
    "        data['lbh'].append(phrase_functions)\n",
    "    elif book in sbh_books:\n",
    "        data['sbh'].append(phrase_functions)\n",
    "        \n",
    "    data[book].append(phrase_functions)\n",
    "    \n",
    "print(f'Done with {i} clauses loaded in data...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adju', 'Cmpl', 'Conj', 'Loca', 'Modi', 'Nega', 'Objc', 'PreC', 'PreO', 'PreS', 'Pred', 'PtcO', 'Rela', 'Subj', 'Time']\n"
     ]
    }
   ],
   "source": [
    "unique_functions = set(F.function.v(phrase) for phrase in F.otype.s('phrase') \n",
    "                           if F.function.v(phrase) not in exclude_functions)\n",
    "unique_functions = sorted(list(unique_functions))\n",
    "nodes =  [\"Clause_Begin\", \"Clause_End\"] + unique_functions\n",
    "print(unique_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transitionWalk(clauses):\n",
    "    states = list()\n",
    "    for clause in clauses:\n",
    "        states.append(\"Clause_Begin\")\n",
    "        states.extend(clause)\n",
    "        states.append(\"Clause_End\")\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MarkovModel(transitionWalk):\n",
    "    transition_Matrix = np.zeros((len(nodes),len(nodes)))\n",
    "    \n",
    "    for i in range(0,len(transitionWalk)-1):\n",
    "        transition_Matrix[nodes.index(transitionWalk[i]),nodes.index(transitionWalk[i+1])] +=1 \n",
    "        \n",
    "    df_Trans = pd.DataFrame(transition_Matrix, columns = nodes, index = nodes)    \n",
    "    \n",
    "     #Remove unused transition functions\n",
    "    df_Trans = df_Trans[(df_Trans.T != 0).any()]\n",
    "    df_Trans = df_Trans.loc[:, (df_Trans != 0).any(axis=0)]\n",
    "    \n",
    "    df_Trans = df_Trans.div(df_Trans.sum(axis=1), axis=0)    \n",
    "    return(df_Trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def totalVariationDistance(df_prob_M1, df_prob_M2):\n",
    "    df_sub = df_prob_M1.subtract(df_prob_M2, fill_value =0)\n",
    "    maxVar = abs(df_sub.values).sum() *0.5\n",
    "    return maxVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hellingerDistance(df_prob_M1,df_prob_M2):\n",
    "    df1 = df_prob_M1.applymap(np.sqrt)\n",
    "    df2 = df_prob_M2.applymap(np.sqrt)\n",
    "    df_sub = df1.subtract(df2, fill_value=0)\n",
    "    df_sub = np.power(df_sub, 2)\n",
    "    sumDiff = np.sum(df_sub.values)\n",
    "    helligerDistance = np.sqrt(sumDiff)/np.sqrt(2)\n",
    "    return helligerDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_fold = dict()\n",
    "for bookname, clauses in data.items():\n",
    "    \n",
    "    kf = KFold(n_splits=2)\n",
    "    \n",
    "    clauses = np.array(clauses)\n",
    "    distances = list()\n",
    "    for train_index, test_index in kf.split(clauses):\n",
    "        train, test = clauses[train_index], clauses[test_index]\n",
    "        \n",
    "        walk_train = transitionWalk(train)\n",
    "        model_train = MarkovModel(walk_train)\n",
    "        \n",
    "        walk_test = transitionWalk(test)\n",
    "        model_test = MarkovModel(walk_test)\n",
    "        \n",
    "        dist = hellingerDistance(model_train,model_test)\n",
    "        distances.append(dist)\n",
    "    K_fold[bookname] = round(np.mean(distances),3)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_Chronicles': 1.2989999999999999,\n",
       " '1_Kings': 1.0980000000000001,\n",
       " '1_Samuel': 1.125,\n",
       " '2_Chronicles': 1.1910000000000001,\n",
       " '2_Kings': 1.147,\n",
       " '2_Samuel': 1.268,\n",
       " 'Deuteronomy': 1.8600000000000001,\n",
       " 'Esther': 1.498,\n",
       " 'Exodus': 1.1459999999999999,\n",
       " 'Ezra': 1.9470000000000001,\n",
       " 'Genesis': 0.89600000000000002,\n",
       " 'Joshua': 1.1699999999999999,\n",
       " 'Judges': 1.161,\n",
       " 'Leviticus': 2.121,\n",
       " 'Nehemiah': 1.351,\n",
       " 'lbh': 1.173,\n",
       " 'sbh': 0.46100000000000002}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
